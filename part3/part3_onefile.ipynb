{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b1c72a-30ff-4727-a09d-785cf4fea38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramGrid Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9855306974344542\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover, ChiSqSelector, IDF, Normalizer\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create a SparkSession\n",
    "#spark = SparkSession.builder.master(\"local[1]\").appName(\"part15\").getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkJob\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the JSON file\n",
    "file = open(\"./reviews_devset copy.json\")\n",
    "reviews_devset = file.readlines()\n",
    "reviewsDF = pd.DataFrame.from_records(list(map(json.loads, reviews_devset)))\n",
    "\n",
    "# Read the JSON file\n",
    "# reviews_devset = spark.read.json(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "# reviewsDF = reviews_devset.toPandas()\n",
    "\n",
    "# Categorize the labels\n",
    "reviewsDF[\"category\"] = pd.Categorical(reviewsDF[\"category\"])\n",
    "reviewsDF[\"labels\"] = reviewsDF[\"category\"].cat.codes\n",
    "\n",
    "\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(reviewsDF.loc[0:1000, [\"reviewText\", \"category\", \"labels\"]])\n",
    "\n",
    "# Define the pipeline stages\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"rawWords\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", caseSensitive=False)\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=2000)\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=idf.getOutputCol(), outputCol=\"selectedFeatures\", labelCol=\"labels\")\n",
    "normalizer = Normalizer(inputCol=\"selectedFeatures\", outputCol=\"normalizedFeatures\", p=2.0)\n",
    "\n",
    "# Define the classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# Define the OneVsRest classifier\n",
    "oneVsRest = OneVsRest(classifier=classifier, labelCol=\"labels\", featuresCol=\"normalizedFeatures\", predictionCol=\"prediction\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, selector, normalizer, oneVsRest])\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "(trainingData, validationData, testData) = spark_df.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(selector.numTopFeatures, [2000, 500, 100]) \\\n",
    "#     .addGrid(classifier.regParam, [0.1, 0.01, 0.001]) \\\n",
    "#     .addGrid(classifier.standardization, [True, False]) \\\n",
    "#     .addGrid(classifier.maxIter, [10, 20]) \\\n",
    "#     .build()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.regParam, [0.1, 0.01, 0.001]) \\\n",
    "    .addGrid(classifier.standardization, [True, False]) \\\n",
    "    .addGrid(classifier.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "print(\"paramGrid Done!\")\n",
    "# Create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Create the cross-validator\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cvModel = crossval.fit(trainingData)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "predictions = cvModel.transform(validationData)\n",
    "\n",
    "# Evaluate the model using F1 measure\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9c829-5c27-4791-b004-973c3d246556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
